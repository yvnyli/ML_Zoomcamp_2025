{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c11a707-53e1-47bc-a9e4-95f1a885ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44e701c7-c970-4536-b3f9-7ec48e620e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"cleaned_df_features.pkl\", \"rb\") as f:\n",
    "    df, numerics, curated_cat, other_cat = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a4e68ed-4ec3-455e-bca3-da4709ce814c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14514552, 13)\n",
      "(3628639, 13)\n",
      "(2015911, 13)\n"
     ]
    }
   ],
   "source": [
    "# first we split trials\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# 10% for testing\n",
    "[df_full_train,df_test] = train_test_split(df,test_size=0.1,random_state=42)\n",
    "# 72% and 18% for train/val\n",
    "[df_train,df_val] = train_test_split(df_full_train,test_size=0.2,random_state=42)\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c5aa1-cc3a-4428-821a-f3e8e189857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# very much challenged by RAM size right now\n",
    "# try to not hold all data at the same time\n",
    "del df\n",
    "\n",
    "with open(\"df_full_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_full_train, f)\n",
    "del df_full_train\n",
    "\n",
    "with open(\"df_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_train, f)\n",
    "del df_train\n",
    "    \n",
    "with open(\"df_val.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_val, f)\n",
    "del df_val\n",
    "\n",
    "with open(\"df_test.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_test, f)\n",
    "del df_test\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9310493c-3a2c-4d59-b8f4-194db6dfb0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get response variable out\n",
    "\n",
    "y_val = df_val.base_passenger_fare.values\n",
    "y_train = df_train.base_passenger_fare.values\n",
    "\n",
    "df_val = df_val.drop(columns=['base_passenger_fare'])\n",
    "df_train = df_train.drop(columns=['base_passenger_fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80dc9c8d-b1bd-40eb-aa8d-1c37888e5aa9",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m t0 = time.time()\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# curated features only\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m X_full_train = \u001b[43mdv_full\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrecords\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m X_full_val = dv_full.transform(df_val.to_dict(orient=\u001b[33m'\u001b[39m\u001b[33mrecords\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     16\u001b[39m t1 = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Github\\ML_Zoomcamp_2025\\project1\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Github\\ML_Zoomcamp_2025\\project1\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Github\\ML_Zoomcamp_2025\\project1\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\_dict_vectorizer.py:320\u001b[39m, in \u001b[36mDictVectorizer.fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    298\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Learn a list of feature name -> indices mappings and transform X.\u001b[39;00m\n\u001b[32m    299\u001b[39m \n\u001b[32m    300\u001b[39m \u001b[33;03m    Like fit(X) followed by transform(X), but does not require\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    318\u001b[39m \u001b[33;03m        Feature vectors; always 2-d.\u001b[39;00m\n\u001b[32m    319\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfitting\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Github\\ML_Zoomcamp_2025\\project1\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\_dict_vectorizer.py:262\u001b[39m, in \u001b[36mDictVectorizer._transform\u001b[39m\u001b[34m(self, X, fitting)\u001b[39m\n\u001b[32m    260\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m feature_name \u001b[38;5;129;01min\u001b[39;00m vocab:\n\u001b[32m    261\u001b[39m                 indices.append(vocab[feature_name])\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m                 \u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m     indptr.append(\u001b[38;5;28mlen\u001b[39m(indices))\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indptr) == \u001b[32m1\u001b[39m:\n",
      "\u001b[31mMemoryError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# one-hot encoding: fit from training set\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import time\n",
    "\n",
    "# all features\n",
    "dv_full = DictVectorizer(sparse=True)\n",
    "\n",
    "# this takes some time\n",
    "t0 = time.time()\n",
    "\n",
    "# curated features only\n",
    "X_full_train = dv_full.fit_transform(df_train.to_dict(orient='records'))\n",
    "X_full_val = dv_full.transform(df_val.to_dict(orient='records'))\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "print(f\"DV took {(t1-t0)/60:5.2f} min\")\n",
    "print(X_full_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fb75778-d9ae-4115-b710-3869a89c825d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:     16.88 GB\n",
      "Available: 8.35 GB\n",
      "Used:      8.52 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"Total:     {mem.total/1e9:.2f} GB\")\n",
    "print(f\"Available: {mem.available/1e9:.2f} GB\")\n",
    "print(f\"Used:      {mem.used/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b051d54-4fe3-4987-a71a-3aa51470b103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.24e-07\n",
      "8.064982664\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_full_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(aso.asizeof(dv_full) / \u001b[32m1e9\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(aso.asizeof(df_train) / \u001b[32m1e9\u001b[39m)\n\u001b[32m      4\u001b[39m size_bytes = (\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mX_full_train\u001b[49m.data.nbytes +\n\u001b[32m      6\u001b[39m     X_full_train.indices.nbytes +\n\u001b[32m      7\u001b[39m     X_full_train.indptr.nbytes\n\u001b[32m      8\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(size_bytes / \u001b[32m1e9\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_full_train' is not defined"
     ]
    }
   ],
   "source": [
    "import pympler.asizeof as aso\n",
    "print(aso.asizeof(dv_full) / 1e9)\n",
    "print(aso.asizeof(df_train) / 1e9)\n",
    "size_bytes = (\n",
    "    X_full_train.data.nbytes +\n",
    "    X_full_train.indices.nbytes +\n",
    "    X_full_train.indptr.nbytes\n",
    ")\n",
    "print(size_bytes / 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc159ed-5860-4bc4-a6bb-413e9dcbe41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "\n",
    "\n",
    "l1_ratios = [0.0, 0.05, 0.1, 0.2, 0.5]\n",
    "SGDalphas = [1e-5, 1e-4, 1e-3, 1e-2]\n",
    "\n",
    "print(\"\\n=== ElasticNet hyperparameter search ===\")\n",
    "SGD_results = []\n",
    "\n",
    "for alpha in tqdm(SGDalphas, desc=\"SGD ElasticNet alphas\"):\n",
    "    for l1r in tqdm(l1_ratios, desc=\"l1 ratio\", leave=False):\n",
    "    \n",
    "        model_SGD = Pipeline([\n",
    "            (\"scaler\", StandardScaler(with_mean=False)),\n",
    "            (\"reg\", SGDRegressor( \\\n",
    "                penalty=\"elasticnet\",\n",
    "                alpha=alpha,\n",
    "                l1_ratio=l1r,\n",
    "                max_iter=100,\n",
    "                verbose=1)\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        t0 = time.time()\n",
    "        model_SGD.fit(X_full_train, y_train)\n",
    "    \n",
    "        y_pred = model_SGD.predict(X_full_train)\n",
    "        rmse_train = root_mean_squared_error(y_train, y_pred)\n",
    "        \n",
    "        y_pred = model_SGD.predict(X_full_val)\n",
    "        rmse_val = root_mean_squared_error(y_val, y_pred)\n",
    "        \n",
    "        t1 = time.time()\n",
    "        train_time = t1 - t0\n",
    "        \n",
    "        print(f\"alpha={alpha:8.6f} | l1r={l1r:8.4f} | train_RMSE={rmse_train:8.4f} | val_RMSE={rmse_val:8.4f} | time={train_time/60:5.2f} min\")\n",
    "        SGD_results.append((alpha, l1r, rmse_train, rmse_val, train_time, model_lasso))\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"full_SGDEN_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump((SGD_results, l1_ratios, SGDalphas), f)\n",
    "\n",
    "best_SGD_alpha, best_SGD_l1r, best_SGD_rmse_train, best_SGD_rmse_val, best_SGD_time, best_SGD_model = min(\n",
    "    SGD_results,\n",
    "    key=lambda x: x[3]\n",
    ")\n",
    "\n",
    "print(f\"\\nBest SGD alpha: {best_SGD_alpha}\")\n",
    "print(f\"Best SGD L1 ratio: {best_SGD_l1r:.4f}\")\n",
    "print(f\"Best SGD training RMSE: {best_SGD_rmse_train:.4f}\")\n",
    "print(f\"Best SGD validation RMSE: {best_SGD_rmse_val:.4f}\")\n",
    "print(f\"Best SGD training time: {best_SGD_time/60:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
