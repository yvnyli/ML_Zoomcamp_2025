{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c11a707-53e1-47bc-a9e4-95f1a885ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44e701c7-c970-4536-b3f9-7ec48e620e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"cleaned_df_features.pkl\", \"rb\") as f:\n",
    "    df, numerics, curated_cat, other_cat = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a4e68ed-4ec3-455e-bca3-da4709ce814c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14514552, 13)\n",
      "(3628639, 13)\n",
      "(2015911, 13)\n"
     ]
    }
   ],
   "source": [
    "# first we split trials\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# 10% for testing\n",
    "[df_full_train,df_test] = train_test_split(df,test_size=0.1,random_state=42)\n",
    "# 72% and 18% for train/val\n",
    "[df_train,df_val] = train_test_split(df_full_train,test_size=0.2,random_state=42)\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b94c5aa1-cc3a-4428-821a-f3e8e189857b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# very much challenged by RAM size right now\n",
    "# try to not hold all data at the same time\n",
    "del df\n",
    "\n",
    "with open(\"df_full_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_full_train, f)\n",
    "del df_full_train\n",
    "\n",
    "with open(\"df_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_train, f)\n",
    "del df_train\n",
    "    \n",
    "with open(\"df_val.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_val, f)\n",
    "del df_val\n",
    "\n",
    "with open(\"df_test.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_test, f)\n",
    "del df_test\n",
    "\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fa5ea40-956f-4dfa-9e6c-a129639f83f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# very carefully, load only df_train\n",
    "\n",
    "with open(\"df_train.pkl\", \"rb\") as f:\n",
    "    df_train = pickle.load(f)\n",
    "y_train = df_train.base_passenger_fare.values\n",
    "df_train = df_train.drop(columns=['base_passenger_fare'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a581fb3-4644-44cb-9067-a19ca1845c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DV took  3.85 min\n",
      "(14514552, 568)\n",
      "X_full_train is  2.1481537 GBs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encoding\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import time\n",
    "\n",
    "# all features\n",
    "dv_full = DictVectorizer(sparse=True)\n",
    "\n",
    "# this takes some time\n",
    "t0 = time.time()\n",
    "\n",
    "# all features\n",
    "X_full_train = dv_full.fit_transform(df_train.to_dict(orient='records'))\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "del df_train\n",
    "gc.collect()\n",
    "\n",
    "print(f\"DV took {(t1-t0)/60:5.2f} min\")\n",
    "print(X_full_train.shape)\n",
    "\n",
    "\n",
    "with open(\"dv_full.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dv_full, f)\n",
    "\n",
    "\n",
    "import pympler.asizeof as aso\n",
    "size_bytes = (\n",
    "    X_full_train.data.nbytes +\n",
    "    X_full_train.indices.nbytes +\n",
    "    X_full_train.indptr.nbytes\n",
    ")\n",
    "print(\"X_full_train is \", size_bytes / 1e9, \"GBs\")\n",
    "\n",
    "    \n",
    "# deleting for now because I don't want X and df to live in RAM together\n",
    "with open(\"X_full_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_full_train, f)\n",
    "del X_full_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cafb70f5-4398-4042-9908-962ec6d414d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3628639, 568)\n",
      "X_full_val is  0.537038576 GBs\n"
     ]
    }
   ],
   "source": [
    "# now one-hot encode df_val\n",
    "with open(\"df_val.pkl\", \"rb\") as f:\n",
    "    df_val = pickle.load(f)\n",
    "y_val = df_val.base_passenger_fare.values\n",
    "df_val = df_val.drop(columns=['base_passenger_fare'])\n",
    "\n",
    "X_full_val = dv_full.transform(df_val.to_dict(orient='records'))\n",
    "\n",
    "del df_val\n",
    "\n",
    "with open(\"X_full_val.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_full_val, f)\n",
    "\n",
    "    \n",
    "print(X_full_val.shape)\n",
    "import pympler.asizeof as aso\n",
    "size_bytes = (\n",
    "    X_full_val.data.nbytes +\n",
    "    X_full_val.indices.nbytes +\n",
    "    X_full_val.indptr.nbytes\n",
    ")\n",
    "print(\"X_full_val is \", size_bytes / 1e9, \"GBs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fb75778-d9ae-4115-b710-3869a89c825d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total:     16.88 GB\n",
      "Available: 11.09 GB\n",
      "Used:      5.78 GB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"Total:     {mem.total/1e9:.2f} GB\")\n",
    "print(f\"Available: {mem.available/1e9:.2f} GB\")\n",
    "print(f\"Used:      {mem.used/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e21ac82-cf0d-48ff-9ae6-0291926f74aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"X_full_train.pkl\", \"rb\") as f:\n",
    "    X_full_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c6b11a5-0397-46ec-b18d-36ae667d89e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable                  Type              Data/Info\n",
      "-----------------------------------------------------\n",
      "DictVectorizer            type              <class 'sklearn.feature_e<...>ctorizer.DictVectorizer'>\n",
      "KFold                     ABCMeta           <class 'sklearn.model_selection._split.KFold'>\n",
      "Pipeline                  ABCMeta           <class 'sklearn.pipeline.Pipeline'>\n",
      "SGDRegressor              ABCMeta           <class 'sklearn.linear_mo<...>c_gradient.SGDRegressor'>\n",
      "SGD_results               list              n=3\n",
      "SGDalphas                 list              n=4\n",
      "StandardScaler            type              <class 'sklearn.preproces<...>ng._data.StandardScaler'>\n",
      "X_full_train              csr_matrix        <Compressed Sparse Row sp<...>15\\n  (14514551, 567)\t0.0\n",
      "X_full_val                csr_matrix        <Compressed Sparse Row sp<...>214\\n  (3628638, 567)\t0.0\n",
      "alpha                     float             1e-05\n",
      "aso                       module            <module 'pympler.asizeof'<...>es\\\\pympler\\\\asizeof.py'>\n",
      "curated_cat               list              n=5\n",
      "dv_full                   DictVectorizer    DictVectorizer()\n",
      "f                         BufferedReader    <_io.BufferedReader name='X_full_train.pkl'>\n",
      "gc                        module            <module 'gc' (built-in)>\n",
      "l1_ratios                 list              n=5\n",
      "l1r                       float             0.2\n",
      "mem                       svmem             svmem(total=16875896832, <...>918272, free=11094978560)\n",
      "model_SGD                 Pipeline          Pipeline(steps=[('scaler'<...>lasticnet', verbose=1))])\n",
      "np                        module            <module 'numpy' from 'D:\\<...>ges\\\\numpy\\\\__init__.py'>\n",
      "numerics                  list              n=3\n",
      "other_cat                 list              n=2\n",
      "pd                        module            <module 'pandas' from 'D:<...>es\\\\pandas\\\\__init__.py'>\n",
      "pickle                    module            <module 'pickle' from 'C:<...>thon313\\\\Lib\\\\pickle.py'>\n",
      "psutil                    module            <module 'psutil' from 'C:<...>es\\\\psutil\\\\__init__.py'>\n",
      "rmse_train                float             2473981531274.468\n",
      "rmse_val                  float             1988889233478.4465\n",
      "root_mean_squared_error   function          <function root_mean_squar<...>or at 0x000001FE345C31A0>\n",
      "size_bytes                int               537038576\n",
      "sns                       module            <module 'seaborn' from 'D<...>s\\\\seaborn\\\\__init__.py'>\n",
      "t0                        float             1763411007.115106\n",
      "t1                        float             1763411007.1138246\n",
      "time                      module            <module 'time' (built-in)>\n",
      "tqdm                      type              <class 'tqdm.auto.tqdm'>\n",
      "train_test_split          function          <function train_test_split at 0x000001FE3460D6C0>\n",
      "train_time                float             42.274383306503296\n",
      "y_pred                    ndarray           3628639: 3628639 elems, type `float64`, 29029112 bytes (27.68431854248047 Mb)\n",
      "y_train                   ndarray           14514552: 14514552 elems, type `float64`, 116116416 bytes (110.73724365234375 Mb)\n",
      "y_val                     ndarray           3628639: 3628639 elems, type `float64`, 29029112 bytes (27.68431854248047 Mb)\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fef5835e-5e4f-4627-aa55-28566af23feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['trip_miles', 'trip_miles_log1p', 'trip_time', 'trip_time_log1p',\n",
       "       'wait_time_sec_log1p'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftns_full = dv_full.get_feature_names_out()\n",
    "ftns_full[[562,563,564,565,566]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9de9f7b3-3096-4b8e-90d5-c0d5a3af63a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "||||||| All features |||||||\n",
      "\n",
      "\n",
      "=== Ridge (L2) hyperparameter search ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76269c72fd64934aa99ec571432faa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ridge alphas:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=  0.0000 | train_RMSE=  6.7556 | val_RMSE=  6.7215 | time= 1.02 min\n",
      "alpha=  0.1000 | train_RMSE=  6.7556 | val_RMSE=  6.7215 | time= 0.97 min\n",
      "alpha=  1.0000 | train_RMSE=  6.7556 | val_RMSE=  6.7215 | time= 0.92 min\n",
      "alpha= 10.0000 | train_RMSE=  6.7556 | val_RMSE=  6.7215 | time= 0.86 min\n",
      "alpha=100.0000 | train_RMSE=  6.7556 | val_RMSE=  6.7215 | time= 0.77 min\n",
      "\n",
      "Best Ridge alpha: 0.1\n",
      "Best Ridge training RMSE: 6.7556\n",
      "Best Ridge validation RMSE: 6.7215\n",
      "Best Ridge training time: 0.97 minutes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Optional: progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"\\n||||||| All features |||||||\\n\")\n",
    "\n",
    "# ------------------------------------\n",
    "# 1. Linear models: L2 (Ridge) search\n",
    "# ------------------------------------\n",
    "\n",
    "# Note: with one-hot/sparse features, use with_mean=False\n",
    "# If X_* are sparse, this will keep them sparse where possible.\n",
    "\n",
    "\n",
    "# only scale 3 numeric features\n",
    "numeric_idx = [562,563,564,565,566]\n",
    "all_idx = list(range(568))\n",
    "cat_idx = [j for j in all_idx if j not in numeric_idx]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(with_mean=False), numeric_idx),\n",
    "        (\"cat\", \"passthrough\", cat_idx),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "ridge_alphas = [0.0, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "print(\"\\n=== Ridge (L2) hyperparameter search ===\")\n",
    "\n",
    "ridge_results = []\n",
    "for alpha in tqdm(ridge_alphas, desc=\"Ridge alphas\"):\n",
    "    model_ridge = Pipeline([\n",
    "        (\"scaler\", preprocess),\n",
    "        (\"reg\", Ridge(alpha=alpha, random_state=0))\n",
    "    ])\n",
    "\n",
    "    t0 = time.time()\n",
    "    model_ridge.fit(X_full_train, y_train)\n",
    "\n",
    "    y_pred = model_ridge.predict(X_full_train)\n",
    "    rmse_train = root_mean_squared_error(y_train, y_pred)\n",
    "    \n",
    "    y_pred = model_ridge.predict(X_full_val)\n",
    "    rmse_val = root_mean_squared_error(y_val, y_pred)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    train_time = t1 - t0\n",
    "\n",
    "    print(f\"alpha={alpha:8.4f} | train_RMSE={rmse_train:8.4f} | val_RMSE={rmse_val:8.4f} | time={train_time/60:5.2f} min\")\n",
    "    ridge_results.append((alpha, rmse_train, rmse_val, train_time, model_ridge))\n",
    "\n",
    "# Pick best Ridge by validation RMSE\n",
    "best_ridge_alpha, best_ridge_rmse_train, best_ridge_rmse_val, best_ridge_time, best_ridge_model = min(\n",
    "    ridge_results,\n",
    "    key=lambda x: x[2]\n",
    ")\n",
    "\n",
    "print(f\"\\nBest Ridge alpha: {best_ridge_alpha}\")\n",
    "print(f\"Best Ridge training RMSE: {best_ridge_rmse_train:.4f}\")\n",
    "print(f\"Best Ridge validation RMSE: {best_ridge_rmse_val:.4f}\")\n",
    "print(f\"Best Ridge training time: {best_ridge_time/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20d5f5e-250e-4ab2-a369-569b1b5f43e3",
   "metadata": {},
   "source": [
    "Ridge regression on all features achieved XGB's performance on limited features (32% improvement). But still far from overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59f9fdd0-8044-4269-a940-860db7b567d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "||||||| All features |||||||\n",
      "\n",
      "\n",
      "=== Ridge (L2) hyperparameter search ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c69b7faa304f1eb2facd4f27da73aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Ridge alphas:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha=  0.0000 | train_RMSE=  8.9366 | val_RMSE=  8.9809 | time= 1.18 min\n",
      "alpha=  0.1000 | train_RMSE=  8.9366 | val_RMSE=  8.9809 | time= 0.95 min\n",
      "alpha=  1.0000 | train_RMSE=  8.9366 | val_RMSE=  8.9809 | time= 0.91 min\n",
      "alpha= 10.0000 | train_RMSE=  8.9366 | val_RMSE=  8.9809 | time= 0.90 min\n",
      "alpha=100.0000 | train_RMSE=  8.9367 | val_RMSE=  8.9809 | time= 1.06 min\n",
      "\n",
      "Best Ridge alpha: 0.0\n",
      "Best Ridge training RMSE: 8.9366\n",
      "Best Ridge validation RMSE: 8.9809\n",
      "Best Ridge training time: 1.18 minutes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Optional: progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"\\n||||||| All features |||||||\\n\")\n",
    "\n",
    "# ------------------------------------\n",
    "# 1. Linear models: L2 (Ridge) search\n",
    "# ------------------------------------\n",
    "\n",
    "# Note: with one-hot/sparse features, use with_mean=False\n",
    "# If X_* are sparse, this will keep them sparse where possible.\n",
    "\n",
    "\n",
    "# only scale 3 numeric features\n",
    "numeric_idx = [562,563,564,565,566]\n",
    "all_idx = list(range(568))\n",
    "cat_idx = [j for j in all_idx if j not in numeric_idx]\n",
    "\n",
    "# exclude the non log1p numerics\n",
    "numeric_idx = [563,565,566]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(with_mean=False), numeric_idx),\n",
    "        (\"cat\", \"passthrough\", cat_idx),\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n",
    "\n",
    "\n",
    "ridge_alphas = [0.0, 0.1, 1.0, 10.0, 100.0]\n",
    "\n",
    "print(\"\\n=== Ridge (L2) hyperparameter search ===\")\n",
    "\n",
    "ridge_results = []\n",
    "for alpha in tqdm(ridge_alphas, desc=\"Ridge alphas\"):\n",
    "    model_ridge = Pipeline([\n",
    "        (\"scaler\", preprocess),\n",
    "        (\"reg\", Ridge(alpha=alpha, random_state=0))\n",
    "    ])\n",
    "\n",
    "    t0 = time.time()\n",
    "    model_ridge.fit(X_full_train, y_train)\n",
    "\n",
    "    y_pred = model_ridge.predict(X_full_train)\n",
    "    rmse_train = root_mean_squared_error(y_train, y_pred)\n",
    "    \n",
    "    y_pred = model_ridge.predict(X_full_val)\n",
    "    rmse_val = root_mean_squared_error(y_val, y_pred)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    train_time = t1 - t0\n",
    "\n",
    "    print(f\"alpha={alpha:8.4f} | train_RMSE={rmse_train:8.4f} | val_RMSE={rmse_val:8.4f} | time={train_time/60:5.2f} min\")\n",
    "    ridge_results.append((alpha, rmse_train, rmse_val, train_time, model_ridge))\n",
    "\n",
    "# Pick best Ridge by validation RMSE\n",
    "best_ridge_alpha, best_ridge_rmse_train, best_ridge_rmse_val, best_ridge_time, best_ridge_model = min(\n",
    "    ridge_results,\n",
    "    key=lambda x: x[2]\n",
    ")\n",
    "\n",
    "print(f\"\\nBest Ridge alpha: {best_ridge_alpha}\")\n",
    "print(f\"Best Ridge training RMSE: {best_ridge_rmse_train:.4f}\")\n",
    "print(f\"Best Ridge validation RMSE: {best_ridge_rmse_val:.4f}\")\n",
    "print(f\"Best Ridge training time: {best_ridge_time/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95acb758-2517-47a6-88a6-c4dfde61f101",
   "metadata": {},
   "source": [
    "I realized the first ridge regression contains \"trip_miles\" and \"trip_time\" (in addition to their log1p version).  \n",
    "In the 32% improvement of ridge regression V2 (RMSE=6.72 compared to 10), 22% comes from these two features, and 10% comes from pickup and dropoff location.  \n",
    "This actually makes sense, since they share the skewness as y.  \n",
    "I will keep them for SGD and XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92af6c0f-145c-4fd2-bdb7-bf955dcf2ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "GridSearchCV done in 100.52 minutes\n",
      "SGDEN Best params: {'reg__alpha': 0.001, 'reg__l1_ratio': 0.0}\n",
      "SGDEN Best CV score (RMSE): 7.653158528260437\n",
      "SGDEN search time: 100.52 minutes\n",
      "SGDEN training RMSE (best model): 7.4839\n",
      "SGDEN validation RMSE (best model): 7.4534\n"
     ]
    }
   ],
   "source": [
    "# GridSearchCV version of SGD ElasticNet\n",
    "\n",
    "# from sklearn.linear_model import SGDRegressor\n",
    "# from tqdm.auto import tqdm\n",
    "# from sklearn.metrics import root_mean_squared_error\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# only scale 3 numeric features\n",
    "numeric_idx = [562,563,564,565,566]\n",
    "all_idx = list(range(568))\n",
    "cat_idx = [j for j in all_idx if j not in numeric_idx]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(with_mean=False), numeric_idx),\n",
    "        (\"cat\", \"passthrough\", cat_idx),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "SGDalphas = [1e-5, 1e-4, 1e-3, 1e-2]\n",
    "l1_ratios = [0.0, 0.05, 0.1, 0.2, 0.5]\n",
    "\n",
    "model_SGD = Pipeline([\n",
    "    (\"scaler\", preprocess),\n",
    "    (\"reg\", SGDRegressor( \\\n",
    "        loss=\"squared_error\",\n",
    "        penalty=\"elasticnet\",\n",
    "        eta0=1e-6,\n",
    "        max_iter=100,\n",
    "        shuffle=True,\n",
    "        early_stopping=True,\n",
    "        n_iter_no_change=5,\n",
    "        validation_fraction=0.1,\n",
    "        tol=1e-3,\n",
    "        verbose=0)\n",
    "    )\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"reg__alpha\": SGDalphas,\n",
    "    \"reg__l1_ratio\": l1_ratios,\n",
    "}\n",
    "\n",
    "search = GridSearchCV(\n",
    "    estimator=model_SGD,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=3,               # careful: this is expensive on 14.5M rows\n",
    "    n_jobs=2,           # 1 pipeline at a time; let SGD use threads\n",
    "    verbose=2,\n",
    "    return_train_score=True,\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "search.fit(X_full_train, y_train)\n",
    "t1 = time.time()\n",
    "print(f\"GridSearchCV done in {(t1 - t0)/60:.2f} minutes\")\n",
    "\n",
    "with open(\"full_grid_SGDEN_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump((search, l1_ratios, SGDalphas), f)\n",
    "\n",
    "\n",
    "print(\"SGDEN Best params:\", search.best_params_)\n",
    "print(\"SGDEN Best CV score (RMSE):\", -search.best_score_)\n",
    "print(f\"SGDEN search time: {(t1 - t0)/60:.2f} minutes\")\n",
    "\n",
    "best_sgd = search.best_estimator_\n",
    "\n",
    "    \n",
    "# Evaluate best XGB on validation set explicitly:\n",
    "y_pred_sgd = best_sgd.predict(X_full_train)\n",
    "sgd_train_rmse = root_mean_squared_error(y_train, y_pred_sgd)\n",
    "\n",
    "y_pred_sgd = best_sgd.predict(X_full_val)\n",
    "sgd_val_rmse = root_mean_squared_error(y_val, y_pred_sgd)\n",
    "print(f\"SGDEN training RMSE (best model): {sgd_train_rmse:.4f}\")\n",
    "print(f\"SGDEN validation RMSE (best model): {sgd_val_rmse:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bf12da-a966-48ed-b274-1f54f89a83d7",
   "metadata": {},
   "source": [
    "SGD elastic nest is worse than ridge regression again."
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc4c2d33-8e8f-4478-97e4-cd72bd8d6db7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff351cc1-0938-46c3-8d5c-31a0c41e480d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval (early stopping) on 145146 rows\n",
      "train (CV tuning) on 287389 rows\n",
      "\n",
      "=== XGBoost hyperparameter search ===\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.1, max_depth=8, min_child_weight=50, subsample=0.8;, score=-5.900 total time= 1.4min\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.1, max_depth=8, min_child_weight=50, subsample=0.8;, score=-6.166 total time= 1.5min\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.1, max_depth=8, min_child_weight=50, subsample=0.8;, score=-6.140 total time= 1.2min\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=16, min_child_weight=25, subsample=1.0;, score=-5.991 total time=  17.3s\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=16, min_child_weight=25, subsample=1.0;, score=-6.245 total time=  20.8s\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=16, min_child_weight=25, subsample=1.0;, score=-6.262 total time=  20.4s\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.1, max_depth=16, min_child_weight=8, subsample=1.0;, score=-6.059 total time=  44.2s\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.1, max_depth=16, min_child_weight=8, subsample=1.0;, score=-6.280 total time=  46.6s\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.1, max_depth=16, min_child_weight=8, subsample=1.0;, score=-6.269 total time=  45.9s\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=11, min_child_weight=50, subsample=0.8;, score=-5.940 total time=  40.9s\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=11, min_child_weight=50, subsample=0.8;, score=-6.160 total time= 1.3min\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=11, min_child_weight=50, subsample=0.8;, score=-6.148 total time=  47.3s\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=5, min_child_weight=50, subsample=0.8;, score=-5.955 total time= 1.2min\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=5, min_child_weight=50, subsample=0.8;, score=-6.230 total time= 1.0min\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=5, min_child_weight=50, subsample=0.8;, score=-6.216 total time= 1.0min\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, min_child_weight=50, subsample=0.8;, score=-5.940 total time= 1.9min\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, min_child_weight=50, subsample=0.8;, score=-6.224 total time= 2.2min\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, min_child_weight=50, subsample=0.8;, score=-6.164 total time= 1.4min\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=16, min_child_weight=8, subsample=0.8;, score=-6.039 total time= 1.2min\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=16, min_child_weight=8, subsample=0.8;, score=-6.257 total time= 1.1min\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=16, min_child_weight=8, subsample=0.8;, score=-6.267 total time= 1.0min\n",
      "[CV 1/3] END colsample_bytree=0.3, learning_rate=0.3, max_depth=8, min_child_weight=100, subsample=0.8;, score=-5.932 total time=  49.7s\n",
      "[CV 2/3] END colsample_bytree=0.3, learning_rate=0.3, max_depth=8, min_child_weight=100, subsample=0.8;, score=-6.197 total time= 1.0min\n",
      "[CV 3/3] END colsample_bytree=0.3, learning_rate=0.3, max_depth=8, min_child_weight=100, subsample=0.8;, score=-6.173 total time= 1.3min\n",
      "[CV 1/3] END colsample_bytree=0.3, learning_rate=0.3, max_depth=5, min_child_weight=25, subsample=0.8;, score=-5.967 total time= 1.8min\n",
      "[CV 2/3] END colsample_bytree=0.3, learning_rate=0.3, max_depth=5, min_child_weight=25, subsample=0.8;, score=-6.217 total time= 1.6min\n",
      "[CV 3/3] END colsample_bytree=0.3, learning_rate=0.3, max_depth=5, min_child_weight=25, subsample=0.8;, score=-6.221 total time=  48.4s\n",
      "[CV 1/3] END colsample_bytree=0.3, learning_rate=0.1, max_depth=16, min_child_weight=100, subsample=0.8;, score=-5.878 total time= 2.3min\n",
      "[CV 2/3] END colsample_bytree=0.3, learning_rate=0.1, max_depth=16, min_child_weight=100, subsample=0.8;, score=-6.183 total time= 1.6min\n",
      "[CV 3/3] END colsample_bytree=0.3, learning_rate=0.1, max_depth=16, min_child_weight=100, subsample=0.8;, score=-6.118 total time= 2.3min\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=11, min_child_weight=8, subsample=0.8;, score=-6.096 total time=  15.3s\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=11, min_child_weight=8, subsample=0.8;, score=-6.371 total time=  14.0s\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=11, min_child_weight=8, subsample=0.8;, score=-6.359 total time=  14.9s\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.1, max_depth=8, min_child_weight=25, subsample=1.0;, score=-5.928 total time= 2.2min\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.1, max_depth=8, min_child_weight=25, subsample=1.0;, score=-6.172 total time= 2.5min\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.1, max_depth=8, min_child_weight=25, subsample=1.0;, score=-6.148 total time= 2.2min\n",
      "[CV 1/3] END colsample_bytree=0.3, learning_rate=0.05, max_depth=16, min_child_weight=100, subsample=1.0;, score=-5.863 total time= 4.6min\n",
      "[CV 2/3] END colsample_bytree=0.3, learning_rate=0.05, max_depth=16, min_child_weight=100, subsample=1.0;, score=-6.155 total time= 4.1min\n",
      "[CV 3/3] END colsample_bytree=0.3, learning_rate=0.05, max_depth=16, min_child_weight=100, subsample=1.0;, score=-6.117 total time= 3.4min\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=5, min_child_weight=25, subsample=0.8;, score=-5.996 total time=  28.9s\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=5, min_child_weight=25, subsample=0.8;, score=-6.270 total time=  37.4s\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=5, min_child_weight=25, subsample=0.8;, score=-6.243 total time=  28.7s\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=5, min_child_weight=100, subsample=0.8;, score=-5.953 total time=  52.6s\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=5, min_child_weight=100, subsample=0.8;, score=-6.228 total time= 1.4min\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=5, min_child_weight=100, subsample=0.8;, score=-6.177 total time=  50.2s\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=5, min_child_weight=25, subsample=0.8;, score=-5.959 total time= 3.6min\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=5, min_child_weight=25, subsample=0.8;, score=-6.243 total time= 2.7min\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=5, min_child_weight=25, subsample=0.8;, score=-6.180 total time= 2.8min\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_weight=8, subsample=1.0;, score=-6.008 total time=  40.0s\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_weight=8, subsample=1.0;, score=-6.256 total time= 2.3min\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_weight=8, subsample=1.0;, score=-6.202 total time= 1.8min\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=5, min_child_weight=8, subsample=0.8;, score=-5.960 total time= 2.9min\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=5, min_child_weight=8, subsample=0.8;, score=-6.224 total time= 3.0min\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=5, min_child_weight=8, subsample=0.8;, score=-6.175 total time= 2.4min\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=16, min_child_weight=50, subsample=1.0;, score=-5.977 total time=  22.8s\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=16, min_child_weight=50, subsample=1.0;, score=-6.215 total time=  21.3s\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=16, min_child_weight=50, subsample=1.0;, score=-6.219 total time=  15.4s\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=11, min_child_weight=50, subsample=1.0;, score=-5.881 total time= 3.2min\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=11, min_child_weight=50, subsample=1.0;, score=-6.141 total time= 3.4min\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=11, min_child_weight=50, subsample=1.0;, score=-6.121 total time= 2.4min\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=5, min_child_weight=100, subsample=0.8;, score=-5.894 total time= 4.9min\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=5, min_child_weight=100, subsample=0.8;, score=-6.190 total time= 5.1min\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=5, min_child_weight=100, subsample=0.8;, score=-6.144 total time= 3.3min\n",
      "[CV 1/3] END colsample_bytree=0.3, learning_rate=0.3, max_depth=16, min_child_weight=100, subsample=1.0;, score=-5.924 total time=  40.6s\n",
      "[CV 2/3] END colsample_bytree=0.3, learning_rate=0.3, max_depth=16, min_child_weight=100, subsample=1.0;, score=-6.214 total time=  56.5s\n",
      "[CV 3/3] END colsample_bytree=0.3, learning_rate=0.3, max_depth=16, min_child_weight=100, subsample=1.0;, score=-6.169 total time=  53.1s\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=16, min_child_weight=50, subsample=0.8;, score=-5.951 total time=  17.8s\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=16, min_child_weight=50, subsample=0.8;, score=-6.212 total time=  22.4s\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=16, min_child_weight=50, subsample=0.8;, score=-6.188 total time=  19.5s\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, min_child_weight=25, subsample=0.8;, score=-5.957 total time= 1.8min\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, min_child_weight=25, subsample=0.8;, score=-6.222 total time= 2.1min\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, min_child_weight=25, subsample=0.8;, score=-6.194 total time= 1.4min\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=11, min_child_weight=100, subsample=1.0;, score=-5.893 total time= 1.4min\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=11, min_child_weight=100, subsample=1.0;, score=-6.171 total time= 1.8min\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=11, min_child_weight=100, subsample=1.0;, score=-6.114 total time= 1.9min\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=11, min_child_weight=50, subsample=0.8;, score=-5.913 total time= 1.7min\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=11, min_child_weight=50, subsample=0.8;, score=-6.165 total time= 2.2min\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=11, min_child_weight=50, subsample=0.8;, score=-6.147 total time= 1.4min\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=8, min_child_weight=50, subsample=0.8;, score=-5.971 total time=  25.5s\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=8, min_child_weight=50, subsample=0.8;, score=-6.227 total time=  25.0s\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=8, min_child_weight=50, subsample=0.8;, score=-6.175 total time=  22.0s\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=8, min_child_weight=25, subsample=0.8;, score=-6.013 total time=  27.1s\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=8, min_child_weight=25, subsample=0.8;, score=-6.279 total time=  19.1s\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=8, min_child_weight=25, subsample=0.8;, score=-6.254 total time=  17.4s\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=5, min_child_weight=50, subsample=1.0;, score=-5.893 total time= 8.4min\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=5, min_child_weight=50, subsample=1.0;, score=-6.150 total time=11.8min\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=5, min_child_weight=50, subsample=1.0;, score=-6.118 total time= 6.1min\n",
      "[CV 1/3] END colsample_bytree=0.3, learning_rate=0.05, max_depth=5, min_child_weight=25, subsample=0.8;, score=-5.931 total time= 4.3min\n",
      "[CV 2/3] END colsample_bytree=0.3, learning_rate=0.05, max_depth=5, min_child_weight=25, subsample=0.8;, score=-6.190 total time= 4.9min\n",
      "[CV 3/3] END colsample_bytree=0.3, learning_rate=0.05, max_depth=5, min_child_weight=25, subsample=0.8;, score=-6.185 total time= 3.5min\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=8, min_child_weight=25, subsample=0.8;, score=-5.981 total time=  24.7s\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=8, min_child_weight=25, subsample=0.8;, score=-6.238 total time=  28.2s\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=8, min_child_weight=25, subsample=0.8;, score=-6.243 total time=  29.5s\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_weight=8, subsample=0.8;, score=-6.018 total time=  46.8s\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_weight=8, subsample=0.8;, score=-6.246 total time= 1.2min\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=8, min_child_weight=8, subsample=0.8;, score=-6.249 total time= 1.1min\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=11, min_child_weight=50, subsample=1.0;, score=-5.947 total time=  39.1s\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=11, min_child_weight=50, subsample=1.0;, score=-6.215 total time=  39.4s\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=11, min_child_weight=50, subsample=1.0;, score=-6.179 total time=  44.2s\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.1, max_depth=11, min_child_weight=25, subsample=1.0;, score=-5.932 total time= 1.5min\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.1, max_depth=11, min_child_weight=25, subsample=1.0;, score=-6.180 total time= 1.5min\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.1, max_depth=11, min_child_weight=25, subsample=1.0;, score=-6.184 total time=  53.6s\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=8, min_child_weight=100, subsample=0.8;, score=-5.946 total time=  28.1s\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=8, min_child_weight=100, subsample=0.8;, score=-6.195 total time=  55.9s\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=8, min_child_weight=100, subsample=0.8;, score=-6.166 total time=  44.1s\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=11, min_child_weight=25, subsample=0.8;, score=-5.993 total time=  19.7s\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=11, min_child_weight=25, subsample=0.8;, score=-6.247 total time=  25.0s\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=11, min_child_weight=25, subsample=0.8;, score=-6.249 total time=  24.1s\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=11, min_child_weight=100, subsample=1.0;, score=-5.870 total time= 3.1min\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=11, min_child_weight=100, subsample=1.0;, score=-6.156 total time= 3.9min\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=11, min_child_weight=100, subsample=1.0;, score=-6.095 total time= 4.3min\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=11, min_child_weight=8, subsample=0.8;, score=-6.012 total time=  35.5s\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=11, min_child_weight=8, subsample=0.8;, score=-6.272 total time=  51.3s\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=11, min_child_weight=8, subsample=0.8;, score=-6.255 total time=  44.3s\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, min_child_weight=8, subsample=0.8;, score=-5.981 total time= 1.5min\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, min_child_weight=8, subsample=0.8;, score=-6.263 total time= 1.6min\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, min_child_weight=8, subsample=0.8;, score=-6.206 total time= 1.4min\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=16, min_child_weight=50, subsample=1.0;, score=-5.933 total time=  49.3s\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=16, min_child_weight=50, subsample=1.0;, score=-6.177 total time= 1.2min\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.1, max_depth=16, min_child_weight=50, subsample=1.0;, score=-6.165 total time=  45.6s\n",
      "[CV 1/3] END colsample_bytree=0.3, learning_rate=0.3, max_depth=11, min_child_weight=100, subsample=0.8;, score=-5.935 total time=  43.0s\n",
      "[CV 2/3] END colsample_bytree=0.3, learning_rate=0.3, max_depth=11, min_child_weight=100, subsample=0.8;, score=-6.223 total time=  50.0s\n",
      "[CV 3/3] END colsample_bytree=0.3, learning_rate=0.3, max_depth=11, min_child_weight=100, subsample=0.8;, score=-6.172 total time= 1.1min\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=5, min_child_weight=50, subsample=1.0;, score=-5.917 total time= 1.8min\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=5, min_child_weight=50, subsample=1.0;, score=-6.194 total time= 2.3min\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.3, max_depth=5, min_child_weight=50, subsample=1.0;, score=-6.150 total time= 2.4min\n",
      "[CV 1/3] END colsample_bytree=0.3, learning_rate=0.05, max_depth=8, min_child_weight=50, subsample=1.0;, score=-5.874 total time= 5.3min\n",
      "[CV 2/3] END colsample_bytree=0.3, learning_rate=0.05, max_depth=8, min_child_weight=50, subsample=1.0;, score=-6.147 total time= 5.6min\n",
      "[CV 3/3] END colsample_bytree=0.3, learning_rate=0.05, max_depth=8, min_child_weight=50, subsample=1.0;, score=-6.120 total time= 4.1min\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=8, min_child_weight=100, subsample=1.0;, score=-5.859 total time= 5.1min\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=8, min_child_weight=100, subsample=1.0;, score=-6.149 total time= 6.2min\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=8, min_child_weight=100, subsample=1.0;, score=-6.094 total time= 5.4min\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=16, min_child_weight=50, subsample=0.8;, score=-5.986 total time=  22.6s\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=16, min_child_weight=50, subsample=0.8;, score=-6.211 total time=  21.6s\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.3, max_depth=16, min_child_weight=50, subsample=0.8;, score=-6.177 total time=  17.8s\n",
      "[CV 1/3] END colsample_bytree=0.3, learning_rate=0.3, max_depth=16, min_child_weight=50, subsample=0.8;, score=-5.988 total time=  40.3s\n",
      "[CV 2/3] END colsample_bytree=0.3, learning_rate=0.3, max_depth=16, min_child_weight=50, subsample=0.8;, score=-6.245 total time=  35.9s\n",
      "[CV 3/3] END colsample_bytree=0.3, learning_rate=0.3, max_depth=16, min_child_weight=50, subsample=0.8;, score=-6.218 total time=  34.3s\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=16, min_child_weight=100, subsample=1.0;, score=-5.871 total time= 2.6min\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=16, min_child_weight=100, subsample=1.0;, score=-6.156 total time= 2.5min\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=16, min_child_weight=100, subsample=1.0;, score=-6.099 total time= 2.1min\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=5, min_child_weight=50, subsample=0.8;, score=-5.910 total time= 4.6min\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=5, min_child_weight=50, subsample=0.8;, score=-6.194 total time= 3.8min\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.05, max_depth=5, min_child_weight=50, subsample=0.8;, score=-6.150 total time= 3.1min\n",
      "[CV 1/3] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, min_child_weight=8, subsample=1.0;, score=-5.936 total time= 2.8min\n",
      "[CV 2/3] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, min_child_weight=8, subsample=1.0;, score=-6.209 total time= 3.5min\n",
      "[CV 3/3] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, min_child_weight=8, subsample=1.0;, score=-6.162 total time= 2.0min\n",
      "[CV 1/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=16, min_child_weight=25, subsample=1.0;, score=-5.999 total time= 1.0min\n",
      "[CV 2/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=16, min_child_weight=25, subsample=1.0;, score=-6.215 total time= 1.7min\n",
      "[CV 3/3] END colsample_bytree=0.8, learning_rate=0.05, max_depth=16, min_child_weight=25, subsample=1.0;, score=-6.209 total time=  58.1s\n",
      "XGB Best params: {'subsample': 1.0, 'min_child_weight': 100, 'max_depth': 8, 'learning_rate': 0.05, 'colsample_bytree': 0.6}\n",
      "XGB Best CV score (RMSE): 6.033908959852478\n",
      "XGB search time: 279.01 minutes\n",
      "XGB training RMSE (best model): 6.2519\n",
      "XGB validation RMSE (best model): 6.2885\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3. XGBoost: randomized search\n",
    "# -------------------------------\n",
    "\n",
    "# portion out some data from _train for early stopping\n",
    "X_temp, X_stop_xgb, y_temp, y_stop_xgb = train_test_split(\n",
    "    X_full_train, y_train, test_size=0.01, random_state=42)\n",
    "# use fewer data for training for speed\n",
    "X_temp, X_train_xgb, y_temp, y_train_xgb = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.02, random_state=42)\n",
    "\n",
    "print(f\"eval (early stopping) on {y_stop_xgb.shape[0]:d} rows\")\n",
    "print(f\"train (CV tuning) on {y_train_xgb.shape[0]:d} rows\")\n",
    "\n",
    "# Define parameter distribution\n",
    "param_dist = {\n",
    "    'max_depth': [5,8,11,16],\n",
    "    'learning_rate': [0.3, 0.1, 0.05],\n",
    "    'min_child_weight': [8, 25, 50, 100],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.3, 0.6, 0.8]\n",
    "}\n",
    "\n",
    "# Create XGBClassifier\n",
    "xgb = XGBRegressor(\n",
    "    tree_method=\"hist\",\n",
    "    enable_categorical=True,  # if using pandas categorical dtypes\n",
    "    n_estimators=2000,        # large, rely on early stopping\n",
    "    objective=\"reg:squarederror\",\n",
    "    eval_metric=\"rmse\",\n",
    "    early_stopping_rounds=10,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "fit_params = {\n",
    "    \"eval_set\": [(X_stop_xgb, y_stop_xgb)],\n",
    "    \"verbose\": False,\n",
    "}\n",
    "\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,          # e.g. 50 random trials\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=4,          # shows progress of the search\n",
    "    n_jobs=1,\n",
    "    cv=3   \n",
    ")\n",
    "\n",
    "print(\"\\n=== XGBoost hyperparameter search ===\")\n",
    "t0 = time.time()\n",
    "search.fit(X_train_xgb, y_train_xgb, **fit_params)\n",
    "t1 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"XGB Best params:\", search.best_params_)\n",
    "print(\"XGB Best CV score (RMSE):\", -search.best_score_)\n",
    "print(f\"XGB search time: {(t1 - t0)/60:.2f} minutes\")\n",
    "\n",
    "best_xgb = search.best_estimator_\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"full_xgboost_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump((search, best_xgb, X_stop_xgb, y_stop_xgb, X_train_xgb, y_train_xgb), f)\n",
    "    \n",
    "# Evaluate best XGB on validation set explicitly:\n",
    "y_pred_xgb = best_xgb.predict(X_full_train)\n",
    "xgb_train_rmse = root_mean_squared_error(y_train, y_pred_xgb)\n",
    "\n",
    "y_pred_xgb = best_xgb.predict(X_full_val)\n",
    "xgb_val_rmse = root_mean_squared_error(y_val, y_pred_xgb)\n",
    "print(f\"XGB training RMSE (best model): {xgb_train_rmse:.4f}\")\n",
    "print(f\"XGB validation RMSE (best model): {xgb_val_rmse:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# 4. Summary of model comparison\n",
    "# ------------------------------------\n",
    "\n",
    "#print(\"\\n=== Summary (validation RMSE) ===\")\n",
    "#print(f\"Ridge (L2)   : {best_ridge_rmse:.4f}  (alpha={best_ridge_alpha})\")\n",
    "#print(f\"XGBoost      : {xgb_val_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5745bb99-3c84-457b-a207-6f82224a62f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    50.000000\n",
      "mean      6.114555\n",
      "std       0.049207\n",
      "min       6.033909\n",
      "25%       6.082533\n",
      "50%       6.111909\n",
      "75%       6.147774\n",
      "max       6.275315\n",
      "Name: mean_CV_RMSE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# let's check the results\n",
    "results = search.cv_results_\n",
    "df_results = pd.DataFrame({\n",
    "    \"mean_fit_time\": results[\"mean_fit_time\"],\n",
    "    \"param_subsample\": results[\"param_subsample\"],\n",
    "    \"param_min_child_weight\": results[\"param_min_child_weight\"],\n",
    "    \"param_max_depth\": results[\"param_max_depth\"],\n",
    "    \"param_learning_rate\": results[\"param_learning_rate\"],\n",
    "    \"param_colsample_bytree\": results[\"param_colsample_bytree\"],\n",
    "    \"mean_CV_RMSE\": -results[\"mean_test_score\"],\n",
    "    \"std_test_score\": results[\"std_test_score\"],\n",
    "    \"rank\": results[\"rank_test_score\"],\n",
    "})\n",
    "\n",
    "df_results.sort_values(\"rank\",inplace=True)\n",
    "print(df_results[\"mean_CV_RMSE\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d41ffbe-756c-4355-949c-3d1dedd45ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>mean_CV_RMSE</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>334.947183</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.033909</td>\n",
       "      <td>0.125499</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>225.576398</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.040477</td>\n",
       "      <td>0.122878</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>145.832374</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.042224</td>\n",
       "      <td>0.123151</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>242.381660</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.044872</td>\n",
       "      <td>0.129746</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>298.053448</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.046878</td>\n",
       "      <td>0.122982</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>179.576977</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50</td>\n",
       "      <td>11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.047516</td>\n",
       "      <td>0.117896</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>524.885771</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.053745</td>\n",
       "      <td>0.114318</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>102.579587</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.059208</td>\n",
       "      <td>0.120096</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>123.576959</td>\n",
       "      <td>0.8</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.059644</td>\n",
       "      <td>0.131447</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>81.965478</td>\n",
       "      <td>0.8</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.068593</td>\n",
       "      <td>0.119921</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>105.237353</td>\n",
       "      <td>0.8</td>\n",
       "      <td>50</td>\n",
       "      <td>11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.074986</td>\n",
       "      <td>0.115100</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>267.355064</td>\n",
       "      <td>0.8</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.076060</td>\n",
       "      <td>0.130418</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55.005841</td>\n",
       "      <td>0.8</td>\n",
       "      <td>50</td>\n",
       "      <td>11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.082407</td>\n",
       "      <td>0.100925</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>137.639047</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.082913</td>\n",
       "      <td>0.109704</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>231.518594</td>\n",
       "      <td>0.8</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.084473</td>\n",
       "      <td>0.124945</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>131.224955</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.087036</td>\n",
       "      <td>0.121717</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>55.566617</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.091877</td>\n",
       "      <td>0.112197</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>77.753881</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.098637</td>\n",
       "      <td>0.118007</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>62.035167</td>\n",
       "      <td>0.8</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.100618</td>\n",
       "      <td>0.119680</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>254.024691</td>\n",
       "      <td>0.8</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.101876</td>\n",
       "      <td>0.120669</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>50.044646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.102036</td>\n",
       "      <td>0.127310</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>165.944345</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.102199</td>\n",
       "      <td>0.119341</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>42.718331</td>\n",
       "      <td>0.8</td>\n",
       "      <td>100</td>\n",
       "      <td>8</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.102392</td>\n",
       "      <td>0.111009</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>110.142566</td>\n",
       "      <td>0.8</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.109627</td>\n",
       "      <td>0.122049</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>52.154191</td>\n",
       "      <td>0.8</td>\n",
       "      <td>100</td>\n",
       "      <td>11</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.110014</td>\n",
       "      <td>0.125283</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>40.897281</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50</td>\n",
       "      <td>11</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.113804</td>\n",
       "      <td>0.118533</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>19.956234</td>\n",
       "      <td>0.8</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.116898</td>\n",
       "      <td>0.118003</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>61.799894</td>\n",
       "      <td>0.8</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.119228</td>\n",
       "      <td>0.119198</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>166.698313</td>\n",
       "      <td>0.8</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.119552</td>\n",
       "      <td>0.114729</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>24.230531</td>\n",
       "      <td>0.8</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.124234</td>\n",
       "      <td>0.110442</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>105.815906</td>\n",
       "      <td>0.8</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.124297</td>\n",
       "      <td>0.118732</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>20.703363</td>\n",
       "      <td>0.8</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.124504</td>\n",
       "      <td>0.098884</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>182.728821</td>\n",
       "      <td>0.8</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.127227</td>\n",
       "      <td>0.121871</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>65.039254</td>\n",
       "      <td>0.8</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.133538</td>\n",
       "      <td>0.126729</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>83.539773</td>\n",
       "      <td>0.8</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.135004</td>\n",
       "      <td>0.118922</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.883358</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.137301</td>\n",
       "      <td>0.113211</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>73.755781</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.141158</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>89.867149</td>\n",
       "      <td>0.8</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.149980</td>\n",
       "      <td>0.121639</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>36.835724</td>\n",
       "      <td>0.8</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>6.150487</td>\n",
       "      <td>0.115479</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>27.528931</td>\n",
       "      <td>0.8</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.154040</td>\n",
       "      <td>0.122318</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>95.299465</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.155126</td>\n",
       "      <td>0.106277</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>23.009639</td>\n",
       "      <td>0.8</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.163186</td>\n",
       "      <td>0.120526</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.550407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.166017</td>\n",
       "      <td>0.124130</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>31.711148</td>\n",
       "      <td>0.8</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.169549</td>\n",
       "      <td>0.123175</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>60.788281</td>\n",
       "      <td>0.8</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.170980</td>\n",
       "      <td>0.107851</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>43.692065</td>\n",
       "      <td>0.8</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.179469</td>\n",
       "      <td>0.118799</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>21.284310</td>\n",
       "      <td>0.8</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.182266</td>\n",
       "      <td>0.119797</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>65.035521</td>\n",
       "      <td>0.8</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.187711</td>\n",
       "      <td>0.104979</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45.485958</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.202669</td>\n",
       "      <td>0.101867</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14.774083</td>\n",
       "      <td>0.8</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.8</td>\n",
       "      <td>6.275315</td>\n",
       "      <td>0.126922</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  param_subsample  param_min_child_weight  param_max_depth  \\\n",
       "43     334.947183              1.0                     100                8   \n",
       "36     225.576398              1.0                     100               11   \n",
       "46     145.832374              1.0                     100               16   \n",
       "12     242.381660              1.0                     100               16   \n",
       "42     298.053448              1.0                      50                8   \n",
       "19     179.576977              1.0                      50               11   \n",
       "28     524.885771              1.0                      50                5   \n",
       "24     102.579587              1.0                     100               11   \n",
       "9      123.576959              0.8                     100               16   \n",
       "0       81.965478              0.8                      50                8   \n",
       "25     105.237353              0.8                      50               11   \n",
       "20     267.355064              0.8                     100                5   \n",
       "3       55.005841              0.8                      50               11   \n",
       "11     137.639047              1.0                      25                8   \n",
       "47     231.518594              0.8                      50                5   \n",
       "41     131.224955              1.0                      50                5   \n",
       "39      55.566617              1.0                      50               16   \n",
       "33      77.753881              1.0                      25               11   \n",
       "7       62.035167              0.8                     100                8   \n",
       "29     254.024691              0.8                      25                5   \n",
       "21      50.044646              1.0                     100               16   \n",
       "48     165.944345              1.0                       8                5   \n",
       "34      42.718331              0.8                     100                8   \n",
       "5      110.142566              0.8                      50                5   \n",
       "40      52.154191              0.8                     100               11   \n",
       "32      40.897281              1.0                      50               11   \n",
       "22      19.956234              0.8                      50               16   \n",
       "14      61.799894              0.8                     100                5   \n",
       "17     166.698313              0.8                       8                5   \n",
       "26      24.230531              0.8                      50                8   \n",
       "23     105.815906              0.8                      25                5   \n",
       "44      20.703363              0.8                      50               16   \n",
       "15     182.728821              0.8                      25                5   \n",
       "4       65.039254              0.8                      50                5   \n",
       "8       83.539773              0.8                      25                5   \n",
       "18      19.883358              1.0                      50               16   \n",
       "49      73.755781              1.0                      25               16   \n",
       "38      89.867149              0.8                       8                5   \n",
       "45      36.835724              0.8                      50               16   \n",
       "30      27.528931              0.8                      25                8   \n",
       "16      95.299465              1.0                       8                8   \n",
       "35      23.009639              0.8                      25               11   \n",
       "1       19.550407              1.0                      25               16   \n",
       "13      31.711148              0.8                      25                5   \n",
       "31      60.788281              0.8                       8                8   \n",
       "37      43.692065              0.8                       8               11   \n",
       "27      21.284310              0.8                      25                8   \n",
       "6       65.035521              0.8                       8               16   \n",
       "2       45.485958              1.0                       8               16   \n",
       "10      14.774083              0.8                       8               11   \n",
       "\n",
       "    param_learning_rate  param_colsample_bytree  mean_CV_RMSE  std_test_score  \\\n",
       "43                 0.05                     0.6      6.033909        0.125499   \n",
       "36                 0.05                     0.6      6.040477        0.122878   \n",
       "46                 0.05                     0.6      6.042224        0.123151   \n",
       "12                 0.05                     0.3      6.044872        0.129746   \n",
       "42                 0.05                     0.3      6.046878        0.122982   \n",
       "19                 0.05                     0.6      6.047516        0.117896   \n",
       "28                 0.05                     0.6      6.053745        0.114318   \n",
       "24                 0.10                     0.8      6.059208        0.120096   \n",
       "9                  0.10                     0.3      6.059644        0.131447   \n",
       "0                  0.10                     0.6      6.068593        0.119921   \n",
       "25                 0.05                     0.8      6.074986        0.115100   \n",
       "20                 0.05                     0.6      6.076060        0.130418   \n",
       "3                  0.10                     0.8      6.082407        0.100925   \n",
       "11                 0.10                     0.6      6.082913        0.109704   \n",
       "47                 0.05                     0.6      6.084473        0.124945   \n",
       "41                 0.30                     0.6      6.087036        0.121717   \n",
       "39                 0.10                     0.8      6.091877        0.112197   \n",
       "33                 0.10                     0.6      6.098637        0.118007   \n",
       "7                  0.30                     0.3      6.100618        0.119680   \n",
       "29                 0.05                     0.3      6.101876        0.120669   \n",
       "21                 0.30                     0.3      6.102036        0.127310   \n",
       "48                 0.10                     0.6      6.102199        0.119341   \n",
       "34                 0.30                     0.6      6.102392        0.111009   \n",
       "5                  0.10                     0.8      6.109627        0.122049   \n",
       "40                 0.30                     0.3      6.110014        0.125283   \n",
       "32                 0.30                     0.8      6.113804        0.118533   \n",
       "22                 0.30                     0.6      6.116898        0.118003   \n",
       "14                 0.30                     0.8      6.119228        0.119198   \n",
       "17                 0.05                     0.6      6.119552        0.114729   \n",
       "26                 0.30                     0.8      6.124234        0.110442   \n",
       "23                 0.10                     0.8      6.124297        0.118732   \n",
       "44                 0.30                     0.8      6.124504        0.098884   \n",
       "15                 0.05                     0.8      6.127227        0.121871   \n",
       "4                  0.30                     0.6      6.133538        0.126729   \n",
       "8                  0.30                     0.3      6.135004        0.118922   \n",
       "18                 0.30                     0.8      6.137301        0.113211   \n",
       "49                 0.05                     0.8      6.141158        0.100699   \n",
       "38                 0.10                     0.8      6.149980        0.121639   \n",
       "45                 0.30                     0.3      6.150487        0.115479   \n",
       "30                 0.30                     0.6      6.154040        0.122318   \n",
       "16                 0.10                     0.8      6.155126        0.106277   \n",
       "35                 0.30                     0.6      6.163186        0.120526   \n",
       "1                  0.30                     0.6      6.166017        0.124130   \n",
       "13                 0.30                     0.8      6.169549        0.123175   \n",
       "31                 0.10                     0.8      6.170980        0.107851   \n",
       "37                 0.10                     0.8      6.179469        0.118799   \n",
       "27                 0.30                     0.8      6.182266        0.119797   \n",
       "6                  0.05                     0.8      6.187711        0.104979   \n",
       "2                  0.10                     0.6      6.202669        0.101867   \n",
       "10                 0.30                     0.8      6.275315        0.126922   \n",
       "\n",
       "    rank  \n",
       "43     1  \n",
       "36     2  \n",
       "46     3  \n",
       "12     4  \n",
       "42     5  \n",
       "19     6  \n",
       "28     7  \n",
       "24     8  \n",
       "9      9  \n",
       "0     10  \n",
       "25    11  \n",
       "20    12  \n",
       "3     13  \n",
       "11    14  \n",
       "47    15  \n",
       "41    16  \n",
       "39    17  \n",
       "33    18  \n",
       "7     19  \n",
       "29    20  \n",
       "21    21  \n",
       "48    22  \n",
       "34    23  \n",
       "5     24  \n",
       "40    25  \n",
       "32    26  \n",
       "22    27  \n",
       "14    28  \n",
       "17    29  \n",
       "26    30  \n",
       "23    31  \n",
       "44    32  \n",
       "15    33  \n",
       "4     34  \n",
       "8     35  \n",
       "18    36  \n",
       "49    37  \n",
       "38    38  \n",
       "45    39  \n",
       "30    40  \n",
       "16    41  \n",
       "35    42  \n",
       "1     43  \n",
       "13    44  \n",
       "31    45  \n",
       "37    46  \n",
       "27    47  \n",
       "6     48  \n",
       "2     49  \n",
       "10    50  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad689590-28ec-40f2-a9af-17af920ac438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
